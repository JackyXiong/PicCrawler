拿到题目之后，第一想法是要用到多线程，网页下载和解析，文件下载等。

最后涉及到的标准库有re, os, collections, argparse。第三方库�主要是2个处理网页内容和下载的：request 和 BeautifulSoup

•下面详细说下这个爬虫的思路，主要分为了2个阶段：

  1.用 requests 库爬取了给定链接的网页源码，使用 BeautifulSoup 和 正则表达式将该源码下可用的正常网页链接提取出来，  
  加入到待访问的链接队列中，而将图片链接提取出来进行存储到文件内。

  2.当第一步骤结束，开始利用线程池下载图片，从文件中取出已经获得的图片链接，放入待完成的任务队列，
  每个线程依次提取单个链接进行下载。这一步依然主要使用线程池技术和 requests 库处理。

•另外单独的 options.py 用于处理命令行参数，传递到主程序。使用的是用于参数处理的argparse。

•遇到的问题：
  1.最开始是打算将下载网页解析和图片下载同时进行，但是考虑到网速不适于同时进行网页源码的下载和图片下载，
  于是改用了现在的程序思路。
  
  2.本打算将图片链接数据存储到sqlite 数据库中，但是由于多个线程不能同时对数据库继续写处理，不然会产生 database is locked错误，
  便改用了文件形式，但是数据库无疑是更好的选择，后期时间充裕会重构代码。
  
  3.关于爬取深度的问题：由于个人时间关系没完成深度的处理。
  
  4.其他问题：若爬虫由于多次爬取同一网站导致 IP被封，可用「多 IP 代理」技巧；若是网站又针对爬虫的反盗链，可用urllib2模块对
  cookie,header伪装浏览器，伪造表单登录等技巧。
  
  5.关于程序复用性:还需要在优化代码以提高复用性。
